{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DeepLab Semantic Segmentation Lion.ipynb","provenance":[{"file_id":"1r4u7YtQSXt_JVy2qdHv5XsFXP3TtP8DH","timestamp":1607230701373},{"file_id":"https://github.com/tensorflow/models/blob/master/research/deeplab/deeplab_demo.ipynb","timestamp":1607226174834}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"KFPcBuVFw61h"},"source":["# Overview\n","\n","This colab demonstrates the steps to use the DeepLab model to perform semantic segmentation on a sample input image. Expected outputs are semantic labels overlayed on the sample image.\n","\n","### About DeepLab\n","The models used in this colab perform semantic segmentation. Semantic segmentation models focus on assigning semantic labels, such as sky, person, or car, to multiple objects and stuff in a single image."]},{"cell_type":"markdown","metadata":{"id":"t3ozFsEEP-u_"},"source":["# Instructions\n","<h3><a href=\"https://cloud.google.com/tpu/\"><img valign=\"middle\" src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/tensorflow-without-a-phd/master/tensorflow-rl-pong/images/tpu-hexagon.png\" width=\"50\"></a>  &nbsp;&nbsp;Use a free TPU device</h3>\n","\n","   1. On the main menu, click Runtime and select **Change runtime type**. Set \"TPU\" as the hardware accelerator.\n","   1. Click Runtime again and select **Runtime > Run All**. You can also run the cells manually with Shift-ENTER."]},{"cell_type":"markdown","metadata":{"id":"7cRiapZ1P3wy"},"source":["## Import Libraries"]},{"cell_type":"code","metadata":{"cellView":"code","id":"kAbdmRmvq0Je","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607463442532,"user_tz":360,"elapsed":6162,"user":{"displayName":"Jeffrey Jia","photoUrl":"","userId":"07067674823366105306"}},"outputId":"9030ce2e-d098-4888-99a3-b0939906ead3"},"source":["import os\n","from io import BytesIO\n","import tarfile\n","import tempfile\n","from six.moves import urllib\n","\n","from matplotlib import gridspec\n","from matplotlib import pyplot as plt\n","from matplotlib import cm\n","import numpy as np\n","from PIL import Image\n","\n","%tensorflow_version 1.x\n","import tensorflow as tf\n","import cv2 as cv"],"execution_count":1,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"p47cYGGOQE1W"},"source":["## Import helper methods\n","These methods help us perform the following tasks:\n","* Load the latest version of the pretrained DeepLab model\n","* Load the colormap from the PASCAL VOC dataset\n","* Adds colors to various labels, such as \"pink\" for people, \"green\" for bicycle and more\n","* Visualize an image, and add an overlay of colors on various regions"]},{"cell_type":"code","metadata":{"cellView":"code","id":"vN0kU6NJ1Ye5","executionInfo":{"status":"ok","timestamp":1607463445636,"user_tz":360,"elapsed":596,"user":{"displayName":"Jeffrey Jia","photoUrl":"","userId":"07067674823366105306"}}},"source":["class DeepLabModel(object):\n","  \"\"\"Class to load deeplab model and run inference.\"\"\"\n","\n","  INPUT_TENSOR_NAME = 'ImageTensor:0'\n","  OUTPUT_TENSOR_NAME = 'SemanticPredictions:0'\n","  INPUT_SIZE = 513\n","  FROZEN_GRAPH_NAME = 'frozen_inference_graph'\n","\n","  def __init__(self, tarball_path):\n","    \"\"\"Creates and loads pretrained deeplab model.\"\"\"\n","    self.graph = tf.Graph()\n","\n","    graph_def = None\n","    # Extract frozen graph from tar archive.\n","    tar_file = tarfile.open(tarball_path)\n","    for tar_info in tar_file.getmembers():\n","      if self.FROZEN_GRAPH_NAME in os.path.basename(tar_info.name):\n","        file_handle = tar_file.extractfile(tar_info)\n","        graph_def = tf.GraphDef.FromString(file_handle.read())\n","        break\n","\n","    tar_file.close()\n","\n","    if graph_def is None:\n","      raise RuntimeError('Cannot find inference graph in tar archive.')\n","\n","    with self.graph.as_default():\n","      tf.import_graph_def(graph_def, name='')\n","\n","    self.sess = tf.Session(graph=self.graph)\n","\n","  def run(self, image):\n","    \"\"\"Runs inference on a single image.\n","\n","    Args:\n","      image: A PIL.Image object, raw input image.\n","\n","    Returns:\n","      resized_image: RGB image resized from original input image.\n","      seg_map: Segmentation map of `resized_image`.\n","    \"\"\"\n","    width, height = image.size\n","    resize_ratio = 1.0 * self.INPUT_SIZE / max(width, height)\n","    target_size = (int(resize_ratio * width), int(resize_ratio * height))\n","    resized_image = image.convert('RGB').resize(target_size, Image.ANTIALIAS)\n","    batch_seg_map = self.sess.run(\n","        self.OUTPUT_TENSOR_NAME,\n","        feed_dict={self.INPUT_TENSOR_NAME: [np.asarray(resized_image)]})\n","    seg_map = batch_seg_map[0]\n","    return resized_image, seg_map\n","\n","\n","def create_pascal_label_colormap():\n","  \"\"\"Creates a label colormap used in PASCAL VOC segmentation benchmark.\n","\n","  Returns:\n","    A Colormap for visualizing segmentation results.\n","  \"\"\"\n","  colormap = np.zeros((256, 3), dtype=int)\n","  ind = np.arange(256, dtype=int)\n","\n","  for shift in reversed(range(8)):\n","    for channel in range(3):\n","      colormap[:, channel] |= ((ind >> channel) & 1) << shift\n","    ind >>= 3\n","\n","  return colormap\n","\n","\n","def label_to_color_image(label):\n","  \"\"\"Adds color defined by the dataset colormap to the label.\n","\n","  Args:\n","    label: A 2D array with integer type, storing the segmentation label.\n","\n","  Returns:\n","    result: A 2D array with floating type. The element of the array\n","      is the color indexed by the corresponding element in the input label\n","      to the PASCAL color map.\n","\n","  Raises:\n","    ValueError: If label is not of rank 2 or its value is larger than color\n","      map maximum entry.\n","  \"\"\"\n","  if label.ndim != 2:\n","    raise ValueError('Expect 2-D input label')\n","\n","  colormap = create_pascal_label_colormap()\n","\n","  if np.max(label) >= len(colormap):\n","    raise ValueError('label value too large.')\n","\n","  return colormap[label]\n","\n","\n","def vis_segmentation(image, seg_map):\n","  \"\"\"Visualizes input image, segmentation map and overlay view.\"\"\"\n","  plt.figure(figsize=(15, 5))\n","  grid_spec = gridspec.GridSpec(1, 4, width_ratios=[6, 6, 6, 1])\n","\n","  # plt.subplot(grid_spec[0])\n","  # plt.imshow(image)\n","  # plt.axis('off')\n","  # plt.title('input image')\n","\n","  # for loop if not black, take pixel from input image\n","\n","\n","  # plt.subplot(grid_spec[1])\n","  seg_image = label_to_color_image(seg_map).astype(np.uint8)\n","  # plt.imshow(seg_image)\n","  # plt.axis('off')\n","  # plt.title('segmentation map')\n","\n","  I = np.asarray(seg_image)\n","  old_img = np.asarray(image)\n","\n","  # print(I)\n","  # print(old_img)\n","  # print(I.shape) # 313 x 513 x 3\n","\n","  new_img = np.zeros([I.shape[0], I.shape[1], I.shape[2]])\n","\n","  # print(I)\n","\n","  for i in range(I.shape[0]):\n","    for j in range(I.shape[1]):\n","      if (np.sum(I[i][j]) > 0):\n","        I[i][j] = old_img[i][j]\n","\n","  # pixels = np.array([[[255, 0, 0], [0, 255, 0]], [[0, 0, 255], [255, 255, 0]]])\n","  # I = I / 255\n","  I = Image.fromarray(I.astype('uint8'), 'RGB')\n","  # I = seg_image.putdata(I)\n","\n","  # plt.imshow(old_img)\n","  \n","  \n","  # plt.imshow(new_img)\n","\n","\n","  # plt.subplot(grid_spec[2])\n","  # plt.imshow(I)\n","  # plt.imshow(I, alpha=0.0)\n","  # plt.axis('off')\n","  # plt.title('segmentation overlay')\n","\n","  # unique_labels = np.unique(seg_map)\n","  # ax = plt.subplot(grid_spec[3])\n","  # plt.imshow(\n","  #     FULL_COLOR_MAP[unique_labels].astype(np.uint8), interpolation='nearest')\n","  # ax.yaxis.tick_right()\n","  # plt.yticks(range(len(unique_labels)), LABEL_NAMES[unique_labels])\n","  # plt.xticks([], [])\n","  # ax.tick_params(width=0.0)\n","  # plt.grid('off')\n","\n","\n","  # plt.subplot(grid_spec[4])\n","  # plt.imshow(new_img)\n","  # plt.axis('off')\n","  # plt.show()\n","  return I\n","\n","\n","LABEL_NAMES = np.asarray([\n","    'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n","    'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n","    'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tv'\n","])\n","\n","FULL_LABEL_MAP = np.arange(len(LABEL_NAMES)).reshape(len(LABEL_NAMES), 1)\n","FULL_COLOR_MAP = label_to_color_image(FULL_LABEL_MAP)"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nGcZzNkASG9A"},"source":["## Select a pretrained model\n","We have trained the DeepLab model using various backbone networks. Select one from the MODEL_NAME list."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c4oXKmnjw6i_","executionInfo":{"status":"ok","timestamp":1607463453391,"user_tz":360,"elapsed":1707,"user":{"displayName":"Jeffrey Jia","photoUrl":"","userId":"07067674823366105306"}},"outputId":"d675c1e4-78d4-4592-8dc9-f6ca9b83588f"},"source":["MODEL_NAME = 'mobilenetv2_coco_voctrainaug'  # @param ['mobilenetv2_coco_voctrainaug', 'mobilenetv2_coco_voctrainval', 'xception_coco_voctrainaug', 'xception_coco_voctrainval']\n","\n","_DOWNLOAD_URL_PREFIX = 'http://download.tensorflow.org/models/'\n","_MODEL_URLS = {\n","    'mobilenetv2_coco_voctrainaug':\n","        'deeplabv3_mnv2_pascal_train_aug_2018_01_29.tar.gz',\n","    'mobilenetv2_coco_voctrainval':\n","        'deeplabv3_mnv2_pascal_trainval_2018_01_29.tar.gz',\n","    'xception_coco_voctrainaug':\n","        'deeplabv3_pascal_train_aug_2018_01_04.tar.gz',\n","    'xception_coco_voctrainval':\n","        'deeplabv3_pascal_trainval_2018_01_04.tar.gz',\n","}\n","_TARBALL_NAME = 'deeplab_model.tar.gz'\n","\n","model_dir = tempfile.mkdtemp()\n","tf.gfile.MakeDirs(model_dir)\n","\n","download_path = os.path.join(model_dir, _TARBALL_NAME)\n","print('downloading model, this might take a while...')\n","urllib.request.urlretrieve(_DOWNLOAD_URL_PREFIX + _MODEL_URLS[MODEL_NAME],\n","                   download_path)\n","print('download completed! loading DeepLab model...')\n","\n","MODEL = DeepLabModel(download_path)\n","print('model loaded successfully!')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["downloading model, this might take a while...\n","download completed! loading DeepLab model...\n","model loaded successfully!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SZst78N-4OKO"},"source":["## Run on sample images\n","\n","Select one of sample images (leave `IMAGE_URL` empty) or feed any internet image\n","url for inference.\n","\n","Note that this colab uses single scale inference for fast computation,\n","so the results may slightly differ from the visualizations in the\n","[README](https://github.com/tensorflow/models/blob/master/research/deeplab/README.md) file,\n","which uses multi-scale and left-right flipped inputs."]},{"cell_type":"markdown","metadata":{"id":"PGmJwq9rZL6b"},"source":[""]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Q9DpZ5mGfkcK","executionInfo":{"status":"ok","timestamp":1607464151787,"user_tz":360,"elapsed":233793,"user":{"displayName":"Jeffrey Jia","photoUrl":"","userId":"07067674823366105306"}},"outputId":"aad912f6-c571-413d-b95c-546d0d382e60"},"source":["\n","def run_visualization(url):\n","  \"\"\"Inferences DeepLab model and visualizes result.\"\"\"\n","  resized_im, seg_map = MODEL.run(url)\n","  frame = vis_segmentation(resized_im, seg_map)\n","  return frame\n","\n","\n","# image = Image.open('lion.jpg')\n","cap = cv.VideoCapture(\"lion_defenced.mp4\")\n","_, first_frame = cap.read()\n","\n","height,width,layers = first_frame.shape\n","\n","# video = cv.VideoWriter('Lion_bg_seg.mp4',cv.VideoWriter_fourcc(*'MP4V'),30,(513,288))\n","video = cv.VideoWriter('Lion_bg_seg.avi',cv.VideoWriter_fourcc(*'MJPG'),30,(513,288))\n","\n","cont = True\n","# while cont:\n","while(cap.isOpened()):\n","    ret, frame = cap.read()\n","    if ret==True:\n","        frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n","        # frame = Image.fromarray(frame.astype('uint8'), 'RGB')\n","        frame = Image.fromarray(frame)\n","        # print(frame)\n","        # plt.imshow(frame)\n","        # plt.show()\n","        # print(frame)\n","\n","\n","        each_frame = run_visualization(frame)\n","        each_frame = np.asarray(each_frame)\n","        each_frame = cv.cvtColor(each_frame, cv.COLOR_BGR2RGB)\n","        # print(each_frame.shape)\n","        # plt.imshow(each_frame)\n","        # plt.show()\n","\n","        video.write(each_frame)\n","\n","        key = cv.waitKey(5)\n","        if key == 27 or key == 'q':\n","            cont = False\n","            break\n","    else:\n","        break\n","\n","cap.release()\n","video.release()\n","cv.destroyAllWindows()\n","\n","# each_frame = run_visualization(image)\n","# print(\"hi\")\n","# plt.imshow(each_frame)\n","# plt.show()"],"execution_count":7,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:98: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"aUbVoHScTJYe"},"source":["## What's next\n","\n","* Learn about [Cloud TPUs](https://cloud.google.com/tpu/docs) that Google designed and optimized specifically to speed up and scale up ML workloads for training and inference and to enable ML engineers and researchers to iterate more quickly.\n","* Explore the range of [Cloud TPU tutorials and Colabs](https://cloud.google.com/tpu/docs/tutorials) to find other examples that can be used when implementing your ML project.\n","* For more information on running the DeepLab model on Cloud TPUs, see the [DeepLab tutorial](https://cloud.google.com/tpu/docs/tutorials/deeplab).\n"]},{"cell_type":"code","metadata":{"id":"vf9wCHTpO14q"},"source":[""],"execution_count":null,"outputs":[]}]}